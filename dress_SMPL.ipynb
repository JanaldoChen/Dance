{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import trimesh\n",
    "import torch\n",
    "from models.networks.smpl import SMPL\n",
    "from utils.util import load_obj, load_pickle_file, write_pickle_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'data/Multi-Garment_dataset'\n",
    "garment_classes = ['Pants', 'ShortPants', 'ShirtNoCoat', 'TShirtNoCoat', 'LongCoat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96\n"
     ]
    }
   ],
   "source": [
    "people_IDs_list = os.listdir(data_root)\n",
    "print(len(people_IDs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = SMPL(pkl_path='assets/smpl_model.pkl', isHres=True).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_pose_cam_v_personal(smpl, people_ID, device='cuda:0'):\n",
    "\n",
    "    smpl_registration_pkl = load_pickle_file(os.path.join(data_root, people_ID, 'registration.pkl'))\n",
    "        \n",
    "    shape = torch.from_numpy(smpl_registration_pkl['betas']).float().to(device)\n",
    "    pose = torch.from_numpy(smpl_registration_pkl['pose']).float().to(device)\n",
    "    \n",
    "    pose_T = torch.zeros(pose.shape).float().to(device)\n",
    "    \n",
    "    verts_T = smpl(shape[None], pose_T[None])[0]\n",
    "    \n",
    "    cam = torch.zeros(3).float().to(device)\n",
    "    cam[0] = (1 - torch.rand(1) * 0.2) / verts_T[:, 0:1].abs().max()\n",
    "    cam[1] = - (verts_T[:, 0].min() + verts_T[:, 0].max()) / 2\n",
    "    cam[2] = - (verts_T[:, 1].min() + verts_T[:, 1].max()) / 2\n",
    "    \n",
    "    v_personal = torch.zeros(verts_T.shape).to(device)\n",
    "    print(\"---\"+people_ID+\"---\")\n",
    "    for garment_type in garment_classes:\n",
    "        garment_obj_path = os.path.join(data_root, people_ID, garment_type+'.obj')\n",
    "        vert_inds = torch.from_numpy(vert_indices[garment_type])\n",
    "        if os.path.isfile(garment_obj_path):\n",
    "            garment_obj = load_obj(garment_obj_path)\n",
    "            garment_v = torch.from_numpy(garment_obj['vertices']).float().to(device)\n",
    "            v_personal[vert_inds] = garment_v - verts_T[vert_inds]\n",
    "            print(garment_type)\n",
    "    return shape, pose, cam, v_personal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hres(v, f):\n",
    "    \"\"\"\n",
    "    Get an upsampled version of the mesh.\n",
    "    OUTPUT:\n",
    "        - nv: new vertices\n",
    "        - nf: faces of the upsampled\n",
    "        - mapping: mapping from low res to high res\n",
    "    \"\"\"\n",
    "    from opendr.topology import loop_subdivider\n",
    "    (mapping, nf) = loop_subdivider(v, f)\n",
    "    nv = mapping.dot(v.ravel()).reshape(-1, 3)\n",
    "    return (nv, nf, mapping)\n",
    "\n",
    "def get_vt_ft():\n",
    "    vt, ft = load_pickle_file('assets/smpl_vt_ft.pkl')\n",
    "    return vt, ft\n",
    "\n",
    "def get_vt_ft_hres():\n",
    "    vt, ft = get_vt_ft()\n",
    "    vt, ft, _ = get_hres(np.hstack((vt, np.ones((vt.shape[0], 1)))), ft)\n",
    "    return vt[:, :2], ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7576, 2) (13776, 3)\n"
     ]
    }
   ],
   "source": [
    "vt, ft = get_vt_ft()\n",
    "print(vt.shape, ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28920, 2) (55104, 3)\n"
     ]
    }
   ],
   "source": [
    "vt_hres, ft_hres = get_vt_ft_hres()\n",
    "print(vt_hres.shape, ft_hres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_pickle_file('assets/smpl_vt_ft_hres.pkl', (vt_hres, ft_hres))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_indices, fts = load_pickle_file('assets/garment_fts.pkl')\n",
    "fts['naked'] = ft_hres\n",
    "for key in vert_indices:\n",
    "    print(key, vert_indices[key].shape)\n",
    "print('------------')\n",
    "for key in fts:\n",
    "    print(key, fts[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for people_ID in people_IDs_list:\n",
    "    shape, pose, cam, v_personal = get_shape_pose_cam_v_personal(smpl, people_ID)\n",
    "    smpl_registered_pkl = {'betas': shape.cpu().numpy(), 'pose': pose.cpu().numpy(), 'camera': cam.cpu().numpy(), 'v_personal': v_personal.cpu().numpy()}\n",
    "    write_pickle_file(os.path.join(data_root, people_ID, 'smpl_registered.pkl'), smpl_registered_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vis_ID = 0\n",
    "people_ID = people_IDs_list[vis_ID]\n",
    "smpl_registered_pkl = load_pickle_file(os.path.join(data_root, people_ID, 'smpl_registered.pkl'))\n",
    "shape = torch.from_numpy(smpl_registered_pkl['betas']).float().cuda()\n",
    "pose = torch.from_numpy(smpl_registered_pkl['pose']).float().cuda()\n",
    "v_personal = torch.from_numpy(smpl_registered_pkl['v_personal']).float().cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts = smpl(shape[None], pose[None])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mesh = trimesh.Trimesh(vertices=verts.cpu(), faces=smpl.faces_hres.cpu(), process=False)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_personal = smpl(shape[None], pose[None], v_personal[None])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mesh = trimesh.Trimesh(vertices=verts_personal.cpu(), faces=smpl.faces_hres.cpu(), process=False)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
