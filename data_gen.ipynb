{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import trimesh\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from utils.multi_garment_dataset import Multi_Garment_Dataset\n",
    "from utils.util import load_obj, load_pickle_file, write_pickle_file, get_f2vts\n",
    "from models.networks.smpl import SMPL\n",
    "from models.networks.render import SMPLRenderer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = 'data/Multi-Garment_dataset'\n",
    "image_size = 256\n",
    "tex_size = 3\n",
    "batch_size = 2\n",
    "num_frame = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Multi_Garment_Dataset(data_root=data_root, num_frame=num_frame)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "smpl = SMPL(pkl_path='pretrains/smpl_model.pkl', isHres=True).cuda()\n",
    "smpl_renderer = SMPLRenderer(smpl.faces_hres, image_size=image_size, tex_size=tex_size).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape torch.Size([2, 10])\n",
      "poses torch.Size([2, 2, 72])\n",
      "cams torch.Size([2, 2, 3])\n",
      "v_personal torch.Size([2, 27554, 3])\n",
      "uv_image torch.Size([2, 3, 2048, 2048])\n",
      "f2vts torch.Size([2, 55104, 3, 2])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(train_loader):\n",
    "    for key in data:\n",
    "        print(key, data[key].size())\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        shape = data['shape'].cuda()\n",
    "        poses = data['poses'].cuda()\n",
    "        cams = data['cams'].cuda()\n",
    "        v_personal = data['v_personal'].cuda()\n",
    "\n",
    "        pose = poses[:, 0, :].contiguous()\n",
    "        cam = cams[:, 0, :].contiguous()\n",
    "\n",
    "        verts = smpl(shape, pose)\n",
    "        verts_personal = smpl(shape, pose, v_personal)\n",
    "        \n",
    "        verts = smpl_renderer.project_to_image(verts, cam, flip=True, withz=True)\n",
    "        verts_personal = smpl_renderer.project_to_image(verts_personal, cam, flip=True, withz=True)\n",
    "\n",
    "        uv_img = data['uv_image'].cuda()\n",
    "        f2vts = data['f2vts'].cuda()\n",
    "        tex_gt = smpl_renderer.extract_tex(uv_img, smpl_renderer.points_to_sampler(f2vts))\n",
    "\n",
    "\n",
    "        \n",
    "        img_masked, mask = smpl_renderer(verts, tex_gt)\n",
    "        img_masked_personal, mask_personal = smpl_renderer(verts_personal, tex_gt)\n",
    "        \n",
    "        fim, wim = smpl_renderer.render_fim_wim(verts_personal)\n",
    "    \n",
    "    if i >= 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 256, 256]) torch.Size([2, 256, 256, 3])\n"
     ]
    }
   ],
   "source": [
    "print(fim.size(), wim.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(-1, device='cuda:0', dtype=torch.int32) tensor(55103, device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(fim.min(), fim.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.Trimesh(vertices=verts[0].cpu(), faces=smpl.faces_hres.cpu(), process=False)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.Trimesh(vertices=verts_personal[0].cpu(), faces=smpl.faces_hres.cpu(), process=False)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_masked_vis = (img_masked.cpu().numpy()[0] * 255).astype(np.uint8).transpose(1, 2, 0)\n",
    "plt.imshow(img_masked_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_masked_personal_vis = (img_masked_personal.cpu().numpy()[0] * 255).astype(np.uint8).transpose(1, 2, 0)\n",
    "plt.imshow(img_masked_personal_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = smpl_renderer.extract_tex_from_image(img_masked_personal, verts_personal)\n",
    "print(tex.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_masked_personal_tex, mask_personal_tex = smpl_renderer(verts_personal, tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_masked_personal_tex_vis = (img_masked_personal_tex.cpu().numpy()[0] * 255).astype(np.uint8).transpose(1, 2, 0)\n",
    "plt.imshow(img_masked_personal_tex_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_T = torch.zeros(pose.size()).float().cuda()\n",
    "print(pose_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_T_personal = smpl(shape, pose_T, v_personal)\n",
    "print(verts_T_personal.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.Trimesh(vertices=verts_T_personal[0].cpu(), faces=smpl.faces_hres.cpu(), process=False)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_IDs_list = os.listdir(data_root)\n",
    "print(len(people_IDs_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "garment_classes = ['Pants', 'ShortPants', 'ShirtNoCoat', 'TShirtNoCoat', 'LongCoat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hres(v, f):\n",
    "    \"\"\"\n",
    "    Get an upsampled version of the mesh.\n",
    "    OUTPUT:\n",
    "        - nv: new vertices\n",
    "        - nf: faces of the upsampled\n",
    "        - mapping: mapping from low res to high res\n",
    "    \"\"\"\n",
    "    from opendr.topology import loop_subdivider\n",
    "    (mapping, nf) = loop_subdivider(v, f)\n",
    "    nv = mapping.dot(v.ravel()).reshape(-1, 3)\n",
    "    return (nv, nf, mapping)\n",
    "\n",
    "def get_vt_ft():\n",
    "    vt, ft = load_pickle_file('pretrains/smpl_vt_ft.pkl')\n",
    "    return vt, ft\n",
    "\n",
    "def get_vt_ft_hres():\n",
    "    vt, ft = get_vt_ft()\n",
    "    vt, ft, _ = get_hres(np.hstack((vt, np.ones((vt.shape[0], 1)))), ft)\n",
    "    return vt[:, :2], ft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt, ft = get_vt_ft()\n",
    "print(vt.shape)\n",
    "print(ft.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vt_hres, ft_hres = get_vt_ft_hres()\n",
    "print(vt_hres.shape)\n",
    "print(ft_hres.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vert_indices, fts = load_pickle_file('pretrains/garment_fts.pkl')\n",
    "fts['naked'] = ft_hres\n",
    "for key in vert_indices:\n",
    "    print(key, vert_indices[key].shape)\n",
    "print('------------')\n",
    "for key in fts:\n",
    "    print(key, fts[key].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shape_pose_cam_v_personal(smpl, people_ID, device='cuda:0'):\n",
    "\n",
    "    smpl_registration_pkl = load_pickle_file(os.path.join(data_root, people_ID, 'registration.pkl'))\n",
    "        \n",
    "    shape = torch.from_numpy(smpl_registration_pkl['betas']).float().to(device)\n",
    "    pose = torch.from_numpy(smpl_registration_pkl['pose']).float().to(device)\n",
    "    \n",
    "    pose_T = torch.zeros(pose.shape).float().to(device)\n",
    "    \n",
    "    verts_T = smpl(shape[None], pose_T[None])[0]\n",
    "    \n",
    "    cam = torch.zeros(3).float().to(device)\n",
    "    cam[0] = (1 - torch.rand(1) * 0.2) / verts_T[:, 0:1].abs().max()\n",
    "    cam[1] = - (verts_T[:, 0].min() + verts_T[:, 0].max()) / 2\n",
    "    cam[2] = - (verts_T[:, 1].min() + verts_T[:, 1].max()) / 2\n",
    "    \n",
    "    v_personal = torch.zeros(verts_T.shape).to(device)\n",
    "    print(\"---\"+people_ID+\"---\")\n",
    "    for garment_type in garment_classes:\n",
    "        garment_obj_path = os.path.join(data_root, people_ID, garment_type+'.obj')\n",
    "        vert_inds = torch.from_numpy(vert_indices[garment_type])\n",
    "        if os.path.isfile(garment_obj_path):\n",
    "            garment_obj = load_obj(garment_obj_path)\n",
    "            garment_v = torch.from_numpy(garment_obj['vertices']).float().to(device)\n",
    "            v_personal[vert_inds] = garment_v - verts_T[vert_inds]\n",
    "            print(garment_type)\n",
    "    return shape, pose, cam, v_personal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.util import write_pickle_file\n",
    "for people_ID in people_IDs_list:\n",
    "    shape, pose, cam, v_personal = get_shape_pose_cam_v_personal(smpl, people_ID)\n",
    "    smpl_registered_pkl = {'betas': shape.cpu().numpy(), 'pose': pose.cpu().numpy(), 'camera': cam.cpu().numpy(), 'v_personal': v_personal.cpu().numpy()}\n",
    "    write_pickle_file(os.path.join(data_root, people_ID, 'smpl_registered.pkl'), smpl_registered_pkl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "people_ID = people_IDs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape, pose, cam, v_personal = get_shape_pose_cam_v_personal(smpl, people_ID)\n",
    "print(shape.size(), pose.size(), cam.size(), v_personal.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_T = torch.zeros(pose.shape).float().cuda()\n",
    "print(pose_T.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_T_personal = smpl(shape[None], pose_T[None], v_personal[None])\n",
    "print(verts_T_personal.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.Trimesh(vertices=verts_T_personal[0].cpu(), faces=smpl.faces_hres.cpu(), process=False)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verts_personal = smpl(shape[None], pose[None], v_personal[None])\n",
    "print(verts_personal.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mesh = trimesh.Trimesh(vertices=verts_personal[0].cpu(), faces=smpl.faces_hres.cpu(), process=False)\n",
    "mesh.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv_img = Image.open(os.path.join(data_root, people_ID, 'registered_tex.jpg')).convert('RGB')\n",
    "plt.imshow(uv_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uv_img = transforms.ToTensor()(uv_img).cuda()\n",
    "print(uv_img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f2vts = get_f2vts(os.path.join(data_root, people_ID, 'smpl_registered.obj'))\n",
    "f2vts = torch.from_numpy(f2vts).float().cuda()\n",
    "print(f2vts.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tex = smpl_renderer.extract_tex(uv_img[None], smpl_renderer.f2vts_to_sampler(f2vts[None]))\n",
    "print(tex.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_T = smpl_renderer.render(verts_T_personal, cam[None], tex)\n",
    "print(img_T.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_T_vis = (img_T.detach().cpu().numpy()[0] * 255).astype(np.uint8).transpose(1, 2, 0)\n",
    "plt.imshow(img_T_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = smpl_renderer.render(verts_personal, cam[None], tex)\n",
    "print(img.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_vis = (img.detach().cpu().numpy()[0] * 255).astype(np.uint8).transpose(1, 2, 0)\n",
    "plt.imshow(img_vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
